{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我是進階的MongoDB 喔！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Aggregation Framework \n",
    "### Q: WHO TWEETED MOST:\n",
    "1. Group tweets by user\n",
    "2. Count each user's tweets\n",
    "3. Select user with the most\n",
    "\n",
    "* Sort into descending order \n",
    "* Select user at top\n",
    "\n",
    "select user with the most = sort into descending order then select user at the top.\n",
    "#### the content in aggregate() is called pipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_tweets():\n",
    "    result = db.tweets.aggregate([\n",
    "        {\"$group\" : {\"_id\" : \"$user.screen_name\",  # _id is the key on different document # user is sub-document\n",
    "                     \"count\" : {\"$sum\" : 1} } }, # sum by 1\n",
    "        {\"$sort\" : {\"count\" : -1}}\n",
    "    ])\n",
    "    return result\n",
    "\n",
    "# _id is the key means that new document has key=_id, value=content of field <.screen_name>\n",
    "# \"\"$group\" operator mean aggregate the same document by use.screen_name,\n",
    "# $sum : sum operator\n",
    "# \"$\" infornt of use.screen_name is not operator\n",
    "# \"$sort\" means sorting the \"count\" number descendingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "The tweets in our twitter collection have a field called \"source\". This field describes the application\n",
    "that was used to create the tweet. Following the examples for using the $group operator, your task is \n",
    "to modify the 'make-pipeline' function to identify most used applications for creating tweets. \n",
    "As a check on your query, 'web' is listed as the most frequently used application.\n",
    "'Ubertwitter' is the second most used. The number of counts should be stored in a field named 'count'\n",
    "(see the assertion at the end of the script).\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline\n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. \n",
    "If you want to run this code locally on your machine, you have to install MongoDB, \n",
    "download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset \n",
    "used in examples in this lesson. \n",
    "If you attempt some of the same queries that we looked at in the lesson examples,\n",
    "your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [{\"$group\" : { \"_id\" : \"$source\",  # notice that add \"$\" before field. that means we tell the mongo read the value in source\n",
    "                              \"count\" : {\"$sum\" : 1}}},\n",
    "                 {\"$sort\" : {\"count\" : -1}}]\n",
    "    return pipeline\n",
    "\n",
    "def tweet_sources(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = tweet_sources(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0] == {u'count': 868, u'_id': u'web'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Operation - An Overview\n",
    "- \\$project : only project some of field \n",
    "- \\$match : match some document into aggregate\n",
    "- \\$group\n",
    "- \\$sum\n",
    "- \\$skip : skip three document, keep the firth document into aggregation\n",
    "- \\$limit : limit the three top document, also call the inversw skip\n",
    "- \\$unwind : if the field has an array with three values, and then unwund it for dividing three values into three different document with the same other contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\$match and \\$project and \\$divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: who has the hightest followers to friends ratio \n",
    "\n",
    "def highest_ration():\n",
    "    result = db.tweets.aggregate([\n",
    "        {\"$macth\" : {\"user.friends_count\" : {\"$gt\" : 0}}},\n",
    "        {\"$project\" : {\"ratio\" : {\"$divide\" : [\"$user.followers_count\",  # $divide operator 除法, divide folloewers and friends \n",
    "                                               \"$user.friends_count\"]},\n",
    "                      \"screen_name\" : \"$user.screen_name\"}}, \n",
    "        # the new stage has tow sub-document which is ratio and screen_name \n",
    "        {\"$sort\" : {\"ratio\" : -1} },\n",
    "        {\"$limit\" : 1}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Write an aggregation query to answer this question:\n",
    "\n",
    "Of the users in the \"Brasilia\" timezone who have tweeted 100 times or more,\n",
    "who has the largest number of followers?\n",
    "\n",
    "The following hints will help you solve this problem:\n",
    "- Time zone is found in the \"time_zone\" field of the user object in each tweet.\n",
    "- The number of tweets for each user is found in the \"statuses_count\" field.\n",
    "  To access these fields you will need to use dot notation (from Lesson 4)\n",
    "- Your aggregation query should return something like the following:\n",
    "{u'ok': 1.0,\n",
    " u'result': [{u'_id': ObjectId('52fd2490bac3fa1975477702'),\n",
    "                  u'followers': 2597,\n",
    "                  u'screen_name': u'marbles',\n",
    "                  u'tweets': 12334}]}\n",
    "Note that you will need to create the fields 'followers', 'screen_name' and 'tweets'.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson,\n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code\n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used \n",
    "in examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [{\"$match\" : {\"user.time_zone\" : \"Brasilia\",  # Be careful the key, since \"time_zone\" is a sub-document under user\n",
    "                             \"user.statuses_count\" : {\"$gte\" : 100} } },\n",
    "                {\"$project\" : {\"followers\" :  \"$user.followers_count\",\n",
    "                               \"screen_name\" : \"$user.screen_name\",\n",
    "                               \"tweets\" : \"$user.statuses_count\"} },            \n",
    "                {\"$sort\" : {\"followers\" : -1} },\n",
    "                {\"$limit\" : 1}]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.tweets.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 1\n",
    "    assert result[0][\"followers\"] == 17209\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\$unwind\n",
    "Who included the most user mentions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_mentions():\n",
    "    results = db.tweets.aggregate([\n",
    "        {\"$unwind\" : \"$entities.user_mentions\"},\n",
    "        {\"$group\" : {\"_id\" : \"$user.screen_name\",\n",
    "                     \"count\" : {\"$sum\" : 1} } },\n",
    "        {\"$sort\" : {\"count\" : -1 } },\n",
    "        {\"$limit\" : 1} ] )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "For this exercise, let's return to our cities infobox dataset. The question we would like you to answer\n",
    "is as follows:  Which region or district in India contains the most cities? (Make sure that the count of\n",
    "cities is stored in a field named 'count'; see the assertions at the end of the script.)\n",
    "\n",
    "As a starting point, use the solution for the example question we looked at -- \"Who includes the most\n",
    "user mentions in their tweets?\"\n",
    "\n",
    "One thing to note about the cities data is that the \"isPartOf\" field contains an array of regions or \n",
    "districts in which a given city is found. See the example document in Instructor Comments below.\n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation pipeline \n",
    "that can be passed to the MongoDB aggregate function. As in our examples in this lesson, the aggregation \n",
    "pipeline should be a list of one or more dictionary objects. Please review the lesson examples if you \n",
    "are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the cities collection used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results may be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [{\"$match\" : {\"country\" : \"India\"}},\n",
    "                {\"$unwind\" : \"$isPartOf\"},\n",
    "                {\"$group\" : {\"_id\" : \"$isPartOf\",  #the \"isPartOf\" field contains an array of regions or districts \n",
    "                             \"count\" : {\"$sum\" : 1}}},\n",
    "                {\"$sort\" : {\"count\" : -1 } },\n",
    "                {\"$limit\" : 1}\n",
    "                ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.cities.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('examples')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    print \"Printing the first result:\"\n",
    "    import pprint\n",
    "    pprint.pprint(result[0])\n",
    "    assert result[0][\"_id\"] == \"Uttar Pradesh\" # the answer is in \"$isPartOf\"\n",
    "    assert result[0][\"count\"] == 623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\$group operators\n",
    "- \\$sum\n",
    "- \\$frist\n",
    "- \\$last\n",
    "- \\$max\n",
    "- \\$min\n",
    "- \\$avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average\n",
    "def hashtag_retweet_avg():\n",
    "    result = db.tweets.aggregate([\n",
    "        {\"$unwind\" : \"$entities.hashtags\"},\n",
    "        {\"$group\" : {\"_id\" : \"$entities.hashtags.text\",\n",
    "                     \"retweet_avg\" : {\"$avg\" : \"$retweet_count\"}\n",
    "                    } },\n",
    "        {\"$sort\" : {\"retweet_avg\" : -1}}\n",
    "    ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deal with array\n",
    "- \\$push : \\$push is similar to $addToSet, it aggregates all values into an array\n",
    "- \\$addToSet :　把 array 重複數字忽略, 唯一的數字記下留在集合裡　(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_hastags_by_user():\n",
    "    results = db.tweets.aggregate([\n",
    "        {\"$unwind\" : \"$entitles.hashtags\"},\n",
    "        {\"group\" : {\"_id\" : \"$user.screen_name\",\n",
    "                    \"unique_hashtags\" : {\n",
    "                        \"$addToSet\" : \"$entities.hashtags.text\"\n",
    "                    } } },\n",
    "        {\"$sort\" : {\"_id\": -1}}\n",
    "    ])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "$push is similar to $addToSet. The difference is that rather than accumulating only unique values \n",
    "it aggregates all values into an array.\n",
    "\n",
    "Using an aggregation query, count the number of tweets for each user. In the same $group stage, \n",
    "use $push to accumulate all the tweet texts for each user. Limit your output to the 5 users\n",
    "with the most tweets. \n",
    "Your result documents should include only the fields:\n",
    "\"_id\" (screen name of user), \n",
    "\"count\" (number of tweets found for the user),\n",
    "\"tweet_texts\" (a list of the tweet texts found for the user).  \n",
    "\n",
    "Please modify only the 'make_pipeline' function so that it creates and returns an aggregation \n",
    "pipeline that can be passed to the MongoDB aggregate function. As in our examples in this lesson, \n",
    "the aggregation pipeline should be a list of one or more dictionary objects. \n",
    "Please review the lesson examples if you are unsure of the syntax.\n",
    "\n",
    "Your code will be run against a MongoDB instance that we have provided. If you want to run this code \n",
    "locally on your machine, you have to install MongoDB, download and insert the dataset.\n",
    "For instructions related to MongoDB setup and datasets please see Course Materials.\n",
    "\n",
    "Please note that the dataset you are using here is a smaller version of the twitter dataset used in \n",
    "examples in this lesson. If you attempt some of the same queries that we looked at in the lesson \n",
    "examples, your results will be different.\n",
    "\"\"\"\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [ \n",
    "        {\"$group\" : {\"_id\" : \"$user.screen_name\", \n",
    "                     \"count\" : {\"$sum\" : 1},  # count how many texts\n",
    "                     \"tweet_texts\" : {\n",
    "                         \"$push\" : \"$text\" # show the content of texts\n",
    "                     } } },\n",
    "        {\"$sort\" : {\"count\" : -1}},\n",
    "        {\"$limit\" : 5}\n",
    "        ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.twitter.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('twitter')\n",
    "    pipeline = make_pipeline()\n",
    "    result = aggregate(db, pipeline)\n",
    "    import pprint\n",
    "    pprint.pprint(result)\n",
    "    assert len(result) == 5\n",
    "    assert result[0][\"count\"] > result[4][\"count\"]\n",
    "    sample_tweet_text = u'Take my money! #liesguystell http://movie.sras2.ayorganes.com'\n",
    "    assert result[4][\"tweet_texts\"][0] == sample_tweet_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple group stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_mention():\n",
    "    result = db.tweets.aggregate([\n",
    "        {\"$unwind\" : \"$entities.user_mentions\"},\n",
    "        {\"group\": {\n",
    "            \"_id\" : \"$user.screen_name\",\n",
    "            \"mest\" : {\n",
    "                \"$addToSet\" : \"$entities.user_mentions.screen_name\"\n",
    "            } } },\n",
    "        {\"$unwind\" : \"$mset\"},\n",
    "        {\"$group\" : {\"_id\" : \"$_id\", \"count\" : {\"$sum\" : 1}}}\n",
    "        {\"$sort\" : {\"count\" : -1 } },\n",
    "        {\"$limit\" : 10}\n",
    "    ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(93599.7764084507 - 201128.0241546919) <  10 ** -8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
